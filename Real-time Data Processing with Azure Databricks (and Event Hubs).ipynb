{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e1a287c0-1c5e-4649-ad56-a7b03707a663",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Importing the libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d376948f-980e-4887-a319-45a525172d43",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b5cc9f6-21cd-4201-a324-e97122c8203f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The code block below creates the catalog and schemas for our solution. \n",
    "\n",
    "The approach utilises a multi-hop data storage architecture (medallion), consisting of bronze, silver, and gold schemas within a 'streaming' catalog. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49397d51-5412-4079-9159-06307954a7c1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check if catalog already exists\n",
      "check if bronze schema already exists\n",
      "check if silver schema already exists\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    spark.sql(\"create catalog streaming MANAGED LOCATION 'abfss://streamingcontainer@streamingstorageaccount.dfs.core.windows.net/';\")\n",
    "except:\n",
    "    print('check if catalog already exists')\n",
    "\n",
    "try:\n",
    "    spark.sql(\"create schema streaming.bronze MANAGED LOCATION 'abfss://streamingcontainer@streamingstorageaccount.dfs.core.windows.net/';\")\n",
    "except:\n",
    "    print('check if bronze schema already exists')\n",
    "\n",
    "try:\n",
    "    spark.sql(\"create schema streaming.silver MANAGED LOCATION 'abfss://streamingcontainer@streamingstorageaccount.dfs.core.windows.net/';\")\n",
    "except:\n",
    "    print('check if silver schema already exists')\n",
    "\n",
    "try:\n",
    "    spark.sql(\"create schema streaming.gold MANAGED LOCATION 'abfss://streamingcontainer@streamingstorageaccount.dfs.core.windows.net/';\")\n",
    "except:\n",
    "    print('check if gold schema already exists')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "90b61da0-3b55-4941-8436-2411646fa4f9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Bronze Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8806de51-7d1e-4dcc-8981-27f5a3447f96",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Set up Azure Event hubs connection string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3397f5f1-841b-4911-9099-8b99b92ab7c0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Config\n",
    "# Replace with your Event Hub namespace, name, and key\n",
    "connectionString = \"***\"\n",
    "eventHubName = \"***\"\n",
    "\n",
    "ehConf = {\n",
    "  'eventhubs.connectionString' : sc._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(connectionString),\n",
    "  'eventhubs.eventHubName': eventHubName\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "37b28ff3-52b8-4a3f-ad67-758854096a2b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Reading and writing the stream to the bronze layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ab57af3-36c7-4ffa-b13f-a833d7a2dc53",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>body</th><th>partition</th><th>offset</th><th>sequenceNumber</th><th>enqueuedTime</th><th>publisher</th><th>partitionKey</th><th>properties</th><th>systemProperties</th></tr></thead><tbody><tr><td>eyJ0ZW1wZXJhdHVyZSI6ICIyNlx1MDBiMEMiLCAidGltZSI6ICJNb25kYXkgMTA6NTdcdTIwMmZwLm0uIiwgInNreWNvbmRpdGlvbiI6ICJDbGVhciJ9</td><td>0</td><td>152</td><td>1</td><td>2024-07-30T02:58:25.463Z</td><td>null</td><td>null</td><td>Map()</td><td>Map(x-opt-sequence-number-epoch -> -1)</td></tr><tr><td>eyJ0ZW1wZXJhdHVyZSI6ICIyNlx1MDBiMEMiLCAidGltZSI6ICJNb25kYXkgMTE6MDhcdTIwMmZwLm0uIiwgInNreWNvbmRpdGlvbiI6ICJDbGVhciJ9</td><td>0</td><td>304</td><td>2</td><td>2024-07-30T03:09:07.561Z</td><td>null</td><td>null</td><td>Map()</td><td>Map(x-opt-sequence-number-epoch -> -1)</td></tr><tr><td>eyJ0ZW1wZXJhdHVyZSI6ICIyNlx1MDBiMEMiLCAidGltZSI6ICJNb25kYXkgMTE6MDlcdTIwMmZwLm0uIiwgInNreWNvbmRpdGlvbiI6ICJDbGVhciJ9</td><td>0</td><td>456</td><td>3</td><td>2024-07-30T03:10:18.235Z</td><td>null</td><td>null</td><td>Map()</td><td>Map(x-opt-sequence-number-epoch -> -1)</td></tr><tr><td>eyJ0ZW1wZXJhdHVyZSI6ICIyNlx1MDBiMEMiLCAidGltZSI6ICJNb25kYXkgMTE6MTBcdTIwMmZwLm0uIiwgInNreWNvbmRpdGlvbiI6ICJDbGVhciJ9</td><td>0</td><td>608</td><td>4</td><td>2024-07-30T03:11:28.439Z</td><td>null</td><td>null</td><td>Map()</td><td>Map(x-opt-sequence-number-epoch -> -1)</td></tr><tr><td>eyJ0ZW1wZXJhdHVyZSI6ICIyNlx1MDBiMEMiLCAidGltZSI6ICJNb25kYXkgMTE6MTFcdTIwMmZwLm0uIiwgInNreWNvbmRpdGlvbiI6ICJDbGVhciJ9</td><td>0</td><td>760</td><td>5</td><td>2024-07-30T03:12:38.301Z</td><td>null</td><td>null</td><td>Map()</td><td>Map(x-opt-sequence-number-epoch -> -1)</td></tr><tr><td>eyJ0ZW1wZXJhdHVyZSI6ICIyNlx1MDBiMEMiLCAidGltZSI6ICJNb25kYXkgMTE6MTJcdTIwMmZwLm0uIiwgInNreWNvbmRpdGlvbiI6ICJDbGVhciJ9</td><td>0</td><td>912</td><td>6</td><td>2024-07-30T03:13:47.943Z</td><td>null</td><td>null</td><td>Map()</td><td>Map(x-opt-sequence-number-epoch -> -1)</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "eyJ0ZW1wZXJhdHVyZSI6ICIyNlx1MDBiMEMiLCAidGltZSI6ICJNb25kYXkgMTA6NTdcdTIwMmZwLm0uIiwgInNreWNvbmRpdGlvbiI6ICJDbGVhciJ9",
         "0",
         "152",
         1,
         "2024-07-30T02:58:25.463Z",
         null,
         null,
         {},
         {
          "x-opt-sequence-number-epoch": "-1"
         }
        ],
        [
         "eyJ0ZW1wZXJhdHVyZSI6ICIyNlx1MDBiMEMiLCAidGltZSI6ICJNb25kYXkgMTE6MDhcdTIwMmZwLm0uIiwgInNreWNvbmRpdGlvbiI6ICJDbGVhciJ9",
         "0",
         "304",
         2,
         "2024-07-30T03:09:07.561Z",
         null,
         null,
         {},
         {
          "x-opt-sequence-number-epoch": "-1"
         }
        ],
        [
         "eyJ0ZW1wZXJhdHVyZSI6ICIyNlx1MDBiMEMiLCAidGltZSI6ICJNb25kYXkgMTE6MDlcdTIwMmZwLm0uIiwgInNreWNvbmRpdGlvbiI6ICJDbGVhciJ9",
         "0",
         "456",
         3,
         "2024-07-30T03:10:18.235Z",
         null,
         null,
         {},
         {
          "x-opt-sequence-number-epoch": "-1"
         }
        ],
        [
         "eyJ0ZW1wZXJhdHVyZSI6ICIyNlx1MDBiMEMiLCAidGltZSI6ICJNb25kYXkgMTE6MTBcdTIwMmZwLm0uIiwgInNreWNvbmRpdGlvbiI6ICJDbGVhciJ9",
         "0",
         "608",
         4,
         "2024-07-30T03:11:28.439Z",
         null,
         null,
         {},
         {
          "x-opt-sequence-number-epoch": "-1"
         }
        ],
        [
         "eyJ0ZW1wZXJhdHVyZSI6ICIyNlx1MDBiMEMiLCAidGltZSI6ICJNb25kYXkgMTE6MTFcdTIwMmZwLm0uIiwgInNreWNvbmRpdGlvbiI6ICJDbGVhciJ9",
         "0",
         "760",
         5,
         "2024-07-30T03:12:38.301Z",
         null,
         null,
         {},
         {
          "x-opt-sequence-number-epoch": "-1"
         }
        ],
        [
         "eyJ0ZW1wZXJhdHVyZSI6ICIyNlx1MDBiMEMiLCAidGltZSI6ICJNb25kYXkgMTE6MTJcdTIwMmZwLm0uIiwgInNreWNvbmRpdGlvbiI6ICJDbGVhciJ9",
         "0",
         "912",
         6,
         "2024-07-30T03:13:47.943Z",
         null,
         null,
         {},
         {
          "x-opt-sequence-number-epoch": "-1"
         }
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "body",
         "type": "\"binary\""
        },
        {
         "metadata": "{}",
         "name": "partition",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "offset",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "sequenceNumber",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "enqueuedTime",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "publisher",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "partitionKey",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "properties",
         "type": "{\"type\":\"map\",\"keyType\":\"string\",\"valueType\":\"string\",\"valueContainsNull\":true}"
        },
        {
         "metadata": "{}",
         "name": "systemProperties",
         "type": "{\"type\":\"map\",\"keyType\":\"string\",\"valueType\":\"string\",\"valueContainsNull\":true}"
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reading stream: Load data from Azure Event Hub into DataFrame 'df' using the previously configured settings\n",
    "df = spark.readStream \\\n",
    "    .format(\"eventhubs\") \\\n",
    "    .options(**ehConf) \\\n",
    "    .load() \\\n",
    "\n",
    "# Displaying stream: Show the incoming streaming data for visualization and debugging purposes\n",
    "df.display()\n",
    "\n",
    "# Writing stream: Persist the streaming data to a Delta table 'streaming.bronze.weather' in 'append' mode with checkpointing\n",
    "df.writeStream\\\n",
    "    .option(\"checkpointLocation\", \"/mnt/streaming/bronze/weather\")\\\n",
    "    .outputMode(\"append\")\\\n",
    "    .format(\"delta\")\\\n",
    "    .toTable(\"streaming.bronze.weather\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dbee364e-6b7e-420f-8b98-8a12d663d880",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Silver Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c824846-2235-4e3b-8e75-68281293d50b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Defining the schema for the JSON object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b661190-f917-448b-9093-1017dde3bb25",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Defining the schema for the JSON object\n",
    "\n",
    "json_schema = StructType([StructField(\"temperature\",StringType(),True),\n",
    "                      StructField(\"time\",StringType(),True),\n",
    "                      StructField(\"skycondition\", StringType(),True)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fcd659c9-65fc-4112-be6e-06f63196311f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Reading, transforming and writing the stream from the bronze to the silver layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18db15d2-f3d1-4dd7-b5aa-ef421208b46c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>temperature</th><th>time</th><th>skycondition</th><th>timestamp</th></tr></thead><tbody><tr><td>26°C</td><td>Monday 10:57 p.m.</td><td>Clear</td><td>2024-07-30T02:58:25.463Z</td></tr><tr><td>26°C</td><td>Monday 11:08 p.m.</td><td>Clear</td><td>2024-07-30T03:09:07.561Z</td></tr><tr><td>26°C</td><td>Monday 11:09 p.m.</td><td>Clear</td><td>2024-07-30T03:10:18.235Z</td></tr><tr><td>26°C</td><td>Monday 11:10 p.m.</td><td>Clear</td><td>2024-07-30T03:11:28.439Z</td></tr><tr><td>26°C</td><td>Monday 11:11 p.m.</td><td>Clear</td><td>2024-07-30T03:12:38.301Z</td></tr><tr><td>26°C</td><td>Monday 11:12 p.m.</td><td>Clear</td><td>2024-07-30T03:13:47.943Z</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "26°C",
         "Monday 10:57 p.m.",
         "Clear",
         "2024-07-30T02:58:25.463Z"
        ],
        [
         "26°C",
         "Monday 11:08 p.m.",
         "Clear",
         "2024-07-30T03:09:07.561Z"
        ],
        [
         "26°C",
         "Monday 11:09 p.m.",
         "Clear",
         "2024-07-30T03:10:18.235Z"
        ],
        [
         "26°C",
         "Monday 11:10 p.m.",
         "Clear",
         "2024-07-30T03:11:28.439Z"
        ],
        [
         "26°C",
         "Monday 11:11 p.m.",
         "Clear",
         "2024-07-30T03:12:38.301Z"
        ],
        [
         "26°C",
         "Monday 11:12 p.m.",
         "Clear",
         "2024-07-30T03:13:47.943Z"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "temperature",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "time",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "skycondition",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "timestamp",
         "type": "\"timestamp\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reading and Transforming: Load streaming data from the 'streaming.bronze.weather' Delta table, cast 'body' to string, parse JSON, and select specific fields\n",
    "df = spark.readStream\\\n",
    "    .format(\"delta\")\\\n",
    "    .table(\"streaming.bronze.weather\")\\\n",
    "    .withColumn(\"body\", col(\"body\").cast(\"string\"))\\\n",
    "    .withColumn(\"body\",from_json(col(\"body\"), json_schema))\\\n",
    "    .select(\"body.temperature\",\"body.time\",\"body.skycondition\", col(\"enqueuedTime\").alias('timestamp'))\n",
    "\n",
    "\n",
    "# Displaying stream: Visualize the transformed data in the DataFrame for verification and analysis\n",
    "df.display()\n",
    "\n",
    "# Writing stream: Save the transformed data to the 'streaming.silver.weather' Delta table in 'append' mode with checkpointing for data reliability\n",
    "df.writeStream\\\n",
    "    .option(\"checkpointLocation\", \"/mnt/streaming/silver/weather\")\\\n",
    "    .outputMode(\"append\")\\\n",
    "    .format(\"delta\")\\\n",
    "    .toTable(\"streaming.silver.weather\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4078a144-9652-490b-adf3-348ad8780e95",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Gold Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53c07779-5a57-4f70-8c90-34554f8a82b1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Reading, aggregating and writing the stream from the silver to the gold layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb4b337b-6bb7-490d-b1d3-2e8b83456661",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Aggregating Stream: Read from 'streaming.silver.weather', apply watermarking and windowing, and calculate average weather metrics\n",
    "df = spark.readStream\\\n",
    "    .format(\"delta\")\\\n",
    "    .table(\"streaming.silver.weather\")\\\n",
    "    .withWatermark(\"timestamp\", \"5 minutes\") \\\n",
    "    .groupBy(window(\"timestamp\", \"5 minutes\")) \\\n",
    "    .agg(avg(\"temperature\").alias('temperature'), avg(\"humidity\").alias('humidity'), avg(\"windSpeed\").alias('windSpeed'), avg(\"precipitation\").alias('precipitation'))\\\n",
    "\t.select('window.start', 'window.end', 'temperature', 'humidity', 'windSpeed', 'precipitation')\n",
    "\n",
    "# Displaying Aggregated Stream: Visualize aggregated data for insights into weather trends\n",
    "df.display()\n",
    "\n",
    "# Writing Aggregated Stream: Store the aggregated data in 'streaming.gold.weather_aggregated' with checkpointing for data integrity\n",
    "df.writeStream\\\n",
    "    .option(\"checkpointLocation\", \"/mnt/streaming/weather_summary\")\\\n",
    "    .outputMode(\"append\")\\\n",
    "    .format(\"delta\")\\\n",
    "    .toTable(\"streaming.gold.weather_summary\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4016826182764860,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Real-time Data Processing with Azure Databricks (and Event Hubs)",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
